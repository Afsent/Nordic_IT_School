{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам дан файл `ru.conversations.txt`\n",
    "\n",
    "В нем содержатся диалоги (разделены пустой строкой).\n",
    "\n",
    "Вам необходимо:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбить текст на диалоги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('ru.conversations.txt', 'r', encoding='utf-8') as f:\n",
    "    text = []\n",
    "    for line in f:\n",
    "        text.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Что,  Мансур,  не жарко теперь тебе?\\n',\n",
       " '- Спрашиваю, не жарко ему теперь?\\n',\n",
       " '\\n',\n",
       " '- Какой полк?\\n',\n",
       " '- Тысяча тридцать четвертый.\\n',\n",
       " '- Вези дальше. Тут тысяча двадцать шестой.\\n',\n",
       " '\\n',\n",
       " '- Какая санрота?\\n',\n",
       " '- Тысяча тридцать шестая.\\n',\n",
       " '- Значит, наша! Принимай тяжелораненого!\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- Что,  Мансур,  не жарко теперь тебе?\\n- Спрашиваю, не жарко ему теперь?\\n', '- Какой полк?\\n- Тысяча тридцать четвертый.\\n- Вези дальше. Тут тысяча двадцать шестой.\\n', '- Какая санрота?\\n- Тысяча тридцать шестая.\\n- Значит, наша! Принимай тяжелораненого!\\n', '- Сколько фрицев в котле?\\n- Тысяч сорок.\\n', '- Чем же он отравился?\\n- А вон, видишь, что-то из тех бутылей выпил.\\n']\n"
     ]
    }
   ],
   "source": [
    "dialogs = []\n",
    "dialog = ''\n",
    "for item in text:\n",
    "    if item != '\\n':\n",
    "        dialog += item\n",
    "    else:\n",
    "        dialogs.append(dialog)\n",
    "        dialog = ''\n",
    "print(dialogs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбить диалоги на токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pro10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(dialogs):\n",
    "    dialogs[i] = word_tokenize(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработать токены\n",
    "Удалить стоп-слова и спецсимволы, лемматизировать или стеммировать слова и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pro10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i,item in enumerate(dialogs):\n",
    "    dialogs[i] = tokenizer.tokenize(str(dialogs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dialogs = dialogs\n",
    "for i, dialog in enumerate(dialogs):\n",
    "    clean_dialog = dialog\n",
    "    for word in dialog:\n",
    "        if word.lower() in stopwords.words('russian'):\n",
    "            clean_dialog.remove(word)\n",
    "    clean_dialogs[i] = clean_dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Мансур', 'жарко', 'тебе', 'Спрашиваю', 'жарко', 'теперь'],\n",
       " ['полк',\n",
       "  'Тысяча',\n",
       "  'тридцать',\n",
       "  'четвертый',\n",
       "  'Вези',\n",
       "  'дальше',\n",
       "  'тысяча',\n",
       "  'двадцать',\n",
       "  'шестой'],\n",
       " ['санрота',\n",
       "  'Тысяча',\n",
       "  'тридцать',\n",
       "  'шестая',\n",
       "  'Значит',\n",
       "  'наша',\n",
       "  'Принимай',\n",
       "  'тяжелораненого'],\n",
       " ['Сколько', 'фрицев', 'котле', 'Тысяч', 'сорок'],\n",
       " ['же', 'отравился', 'вон', 'видишь', 'то', 'тех', 'бутылей', 'выпил']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dialogs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "rus_stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 µs ± 16.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "rus_stemmer.stem('котле')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dialogs = clean_dialogs\n",
    "for i, dialog in enumerate(clean_dialogs):\n",
    "    stem_dialog = dialog\n",
    "    for j, word in enumerate(dialog):\n",
    "        stem_dialog[j] = rus_stemmer.stem(word)\n",
    "    stem_dialogs[i] = stem_dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перевести каждый из диалогов в векторное представление\n",
    "\n",
    "С помощью TF-IDF представить каждый из диалогов в векторном формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'полк тысяч тридца четверт вез дальш тысяч двадца шест'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(map(str, stem_dialogs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]], dtype=int64),\n",
       " 'полк тысяч тридца четверт вез дальш тысяч двадца шест')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "vectorizer.fit([' '.join(map(str, stem_dialogs[1]))])\n",
    "vectors_arr_0 = vectorizer.transform([' '.join(map(str, stem_dialogs[1]))]).toarray()\n",
    "vectors_arr_0, ' '.join(map(str, stem_dialogs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'полк': 6,\n",
       " 'тысяч': 10,\n",
       " 'тридца': 8,\n",
       " 'четверт': 13,\n",
       " 'вез': 0,\n",
       " 'дальш': 2,\n",
       " 'двадца': 4,\n",
       " 'шест': 15,\n",
       " 'полк тысяч': 7,\n",
       " 'тысяч тридца': 12,\n",
       " 'тридца четверт': 9,\n",
       " 'четверт вез': 14,\n",
       " 'вез дальш': 1,\n",
       " 'дальш тысяч': 3,\n",
       " 'тысяч двадца': 11,\n",
       " 'двадца шест': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['полк',\n",
       " 'тысяч',\n",
       " 'тридца',\n",
       " 'четверт',\n",
       " 'вез',\n",
       " 'дальш',\n",
       " 'тысяч',\n",
       " 'двадца',\n",
       " 'шест']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_dialogs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'полк': 3,\n",
       " 'тысяч': 5,\n",
       " 'тридца': 4,\n",
       " 'четверт': 6,\n",
       " 'вез': 0,\n",
       " 'дальш': 1,\n",
       " 'двадца': 2,\n",
       " 'шест': 7}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfvec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25364289 0.25364289 0.25364289 0.25364289 0.25364289 0.25364289\n",
      "  0.         0.         0.         0.         0.25364289 0.25364289\n",
      "  0.         0.         0.         0.         0.180469   0.25364289\n",
      "  0.         0.36093801 0.25364289 0.180469   0.         0.25364289\n",
      "  0.25364289 0.180469   0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.27708406 0.27708406 0.27708406 0.27708406 0.         0.\n",
      "  0.27708406 0.27708406 0.27708406 0.27708406 0.19714759 0.\n",
      "  0.27708406 0.19714759 0.         0.19714759 0.27708406 0.\n",
      "  0.         0.19714759 0.27708406]]\n"
     ]
    }
   ],
   "source": [
    "tfvec = txt.TfidfVectorizer(ngram_range=(1, 2))\n",
    "two_sent_embed = tfvec.fit_transform([' '.join(map(str, stem_dialogs[1])), ' '.join(map(str, stem_dialogs[2]))]).toarray()\n",
    "\n",
    "print(two_sent_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2822674108617793"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(two_sent_embed[0], two_sent_embed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Найти самые близкие по содержанию диалоги, исходя из векторного представления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
